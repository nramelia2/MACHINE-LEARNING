{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Clustering_K-means.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyMkdLRseSzkWCOQndNlV7bO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nramelia2/MACHINE-LEARNING/blob/main/WEEK%205_EDA/Clustering_K_means.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XXbaYkRThcq9"
      },
      "outputs": [],
      "source": [
        "from sklearn.datasets import fetch_20newsgroups # Load the filenames and data from the 20 newsgroups dataset (classification).\n",
        "from sklearn.decomposition import TruncatedSVD\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.feature_extraction.text import HashingVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from sklearn.pipeline import make_pipeline\n",
        "from sklearn.preprocessing import Normalizer\n",
        "from sklearn import metrics\n",
        "\n",
        "from sklearn.cluster import KMeans, MiniBatchKMeans\n",
        "\n",
        "import logging #  provides a set of convenience functions for simple logging usage, example : print()\n",
        "from optparse import OptionParser\n",
        "import sys\n",
        "from time import time\n",
        "\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display progress logs on stdout\n",
        "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")"
      ],
      "metadata": {
        "id": "6oKSF51xhmjQ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### optparse is a more convenient, flexible, and powerful library for parsing command-line options than the old getopt module. optparse uses a more declarative style of command-line parsing: you create an instance of OptionParser, populate it with options, and parse the command line. optparse allows users to specify options in the conventional GNU/POSIX syntax, and additionally generates usage and help messages for you."
      ],
      "metadata": {
        "id": "yKKBJYx1l906"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# parse commandline arguments\n",
        "op = OptionParser()\n",
        "op.add_option(\n",
        "    \"--lsa\",\n",
        "    dest=\"n_components\",\n",
        "    type=\"int\",\n",
        "    help=\"Preprocess documents with latent semantic analysis.\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EKR-RQGbhruI",
        "outputId": "75280035-0c9a-4679-b09d-f8d45caa9b9e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0f09590: --lsa>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.add_option(\n",
        "    \"--no-minibatch\",\n",
        "    action=\"store_false\",\n",
        "    dest=\"minibatch\",\n",
        "    default=True,\n",
        "    help=\"Use ordinary k-means algorithm (in batch mode).\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFgSOX5Khugc",
        "outputId": "bdbabd10-c326-48fe-fbb6-853c2b2f8621"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0f09790: --no-minibatch>"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.add_option(\n",
        "    \"--no-idf\",\n",
        "    action=\"store_false\",\n",
        "    dest=\"use_idf\",\n",
        "    default=True,\n",
        "    help=\"Disable Inverse Document Frequency feature weighting.\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BmwW1w-khwVA",
        "outputId": "ef43cd99-237f-4b6b-f511-3bc35f495c81"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0f09050: --no-idf>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.add_option(\n",
        "    \"--use-hashing\",\n",
        "    action=\"store_true\",\n",
        "    default=False,\n",
        "    help=\"Use a hashing feature vectorizer\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6iYhAbhh0CD",
        "outputId": "f6e09aae-65f3-4861-d330-ad4f9aca11cb"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0f2c750: --use-hashing>"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.add_option(\n",
        "    \"--n-features\",\n",
        "    type=int,\n",
        "    default=10000,\n",
        "    help=\"Maximum number of features (dimensions) to extract from text.\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vkVp0dtsh24o",
        "outputId": "d783b3ee-f5bd-46b3-c6e0-caf332680491"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0f32490: --n-features>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "op.add_option(\n",
        "    \"--verbose\",\n",
        "    action=\"store_true\",\n",
        "    dest=\"verbose\",\n",
        "    default=False,\n",
        "    help=\"Print progress reports inside k-means algorithm.\",\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7mn2Smash5y1",
        "outputId": "c0308e8e-9359-49a5-b9ca-d707b583936d"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Option at 0x7f42c0eba1d0: --verbose>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(__doc__) # command simply re-uses that documentation string to write it to terminal\n",
        "op.print_help()\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1tZsXxpZh8uW",
        "outputId": "5db4e519-8b62-43d4-a643-6d03ea9e8ab5"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Automatically created module for IPython interactive environment\n",
            "Usage: ipykernel_launcher.py [options]\n",
            "\n",
            "Options:\n",
            "  -h, --help            show this help message and exit\n",
            "  --lsa=N_COMPONENTS    Preprocess documents with latent semantic analysis.\n",
            "  --no-minibatch        Use ordinary k-means algorithm (in batch mode).\n",
            "  --no-idf              Disable Inverse Document Frequency feature weighting.\n",
            "  --use-hashing         Use a hashing feature vectorizer\n",
            "  --n-features=N_FEATURES\n",
            "                        Maximum number of features (dimensions) to extract\n",
            "                        from text.\n",
            "  --verbose             Print progress reports inside k-means algorithm.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def is_interactive():\n",
        "    return not hasattr(sys.modules[\"__main__\"], \"__file__\")"
      ],
      "metadata": {
        "id": "Hfd_70vPiBE8"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# work-around for Jupyter notebook and IPython console\n",
        "argv = [] if is_interactive() else sys.argv[1:]\n",
        "(opts, args) = op.parse_args(argv)\n",
        "if len(args) > 0:\n",
        "    op.error(\"this script takes no arguments.\")\n",
        "    sys.exit(1) # there was some issue / error / problem and that is why the program is exiting."
      ],
      "metadata": {
        "id": "6ElSm6LNiDDg"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# Load some categories from the training set\n",
        "categories = [\n",
        "    \"alt.atheism\",\n",
        "    \"talk.religion.misc\",\n",
        "    \"comp.graphics\",\n",
        "    \"sci.space\",\n",
        "]\n",
        "# Uncomment the following to do the analysis on all the categories\n",
        "# categories = None\n",
        "\n",
        "print(\"Loading 20 newsgroups dataset for categories:\")\n",
        "print(categories)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5_sQ-zxiIDO",
        "outputId": "54a255cc-9d87-45ea-c71e-2f059fa53fcc"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 20 newsgroups dataset for categories:\n",
            "['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = fetch_20newsgroups(\n",
        "    subset=\"all\", categories=categories, shuffle=True, random_state=42\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5k1AYBDiNx3",
        "outputId": "65e46441-f7b8-4a7e-f297-08866d2b9ef0"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2022-04-03 13:06:44,189 INFO Downloading 20news dataset. This may take a few minutes.\n",
            "2022-04-03 13:06:44,196 INFO Downloading dataset from https://ndownloader.figshare.com/files/5975967 (14 MB)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"%d documents\" % len(dataset.data))\n",
        "print(\"%d categories\" % len(dataset.target_names))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z7vnn55giQWR",
        "outputId": "8ab71bd6-1ef4-44b1-a7c8-5ce0c95839b7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "3387 documents\n",
            "4 categories\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "labels = dataset.target\n",
        "true_k = np.unique(labels).shape[0]"
      ],
      "metadata": {
        "id": "T2omyg47iT2N"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Extracting features from the training dataset using a sparse vectorizer\")\n",
        "t0 = time()\n",
        "if opts.use_hashing:\n",
        "    if opts.use_idf:\n",
        "        # Perform an IDF normalization on the output of HashingVectorizer\n",
        "        hasher = HashingVectorizer(\n",
        "            n_features=opts.n_features,\n",
        "            stop_words=\"english\",\n",
        "            alternate_sign=False,\n",
        "            norm=None,\n",
        "        )\n",
        "        vectorizer = make_pipeline(hasher, TfidfTransformer())\n",
        "    else:\n",
        "        vectorizer = HashingVectorizer(\n",
        "            n_features=opts.n_features,\n",
        "            stop_words=\"english\",\n",
        "            alternate_sign=False,\n",
        "            norm=\"l2\",\n",
        "        )\n",
        "else:\n",
        "    vectorizer = TfidfVectorizer(\n",
        "        max_df=0.5,\n",
        "        max_features=opts.n_features,\n",
        "        min_df=2,\n",
        "        stop_words=\"english\",\n",
        "        use_idf=opts.use_idf,\n",
        "    )\n",
        "X = vectorizer.fit_transform(dataset.data)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a9yoU50tiZQz",
        "outputId": "24e26c6c-ba08-44ff-d680-d5f790077e2a"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting features from the training dataset using a sparse vectorizer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"done in %fs\" % (time() - t0))\n",
        "print(\"n_samples: %d, n_features: %d\" % X.shape)\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ScZPjyQiibZ4",
        "outputId": "0dc1813f-2ccf-45bb-ea39-2a8ef34285e2"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "done in 16.705478s\n",
            "n_samples: 3387, n_features: 10000\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if opts.n_components:\n",
        "    print(\"Performing dimensionality reduction using LSA\")\n",
        "    t0 = time()\n",
        "    # Vectorizer results are normalized, which makes KMeans behave as\n",
        "    # spherical k-means for better results. Since LSA/SVD results are\n",
        "    # not normalized, we have to redo the normalization.\n",
        "    svd = TruncatedSVD(opts.n_components)\n",
        "    normalizer = Normalizer(copy=False)\n",
        "    lsa = make_pipeline(svd, normalizer)\n",
        "\n",
        "    X = lsa.fit_transform(X)\n",
        "\n",
        "    print(\"done in %fs\" % (time() - t0))\n",
        "\n",
        "    explained_variance = svd.explained_variance_ratio_.sum()\n",
        "    print(\n",
        "        \"Explained variance of the SVD step: {}%\".format(int(explained_variance * 100))\n",
        "    )\n",
        "\n",
        "    print()"
      ],
      "metadata": {
        "id": "5gtVaDv5igUA"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# #############################################################################\n",
        "# Do the actual clustering\n",
        "\n",
        "if opts.minibatch:\n",
        "    km = MiniBatchKMeans(\n",
        "        n_clusters=true_k,\n",
        "        init=\"k-means++\",\n",
        "        n_init=1,\n",
        "        init_size=1000,\n",
        "        batch_size=1000,\n",
        "        verbose=opts.verbose,\n",
        "    )\n",
        "else:\n",
        "    km = KMeans(\n",
        "        n_clusters=true_k,\n",
        "        init=\"k-means++\",\n",
        "        max_iter=100,\n",
        "        n_init=1,\n",
        "        verbose=opts.verbose,\n",
        "    )"
      ],
      "metadata": {
        "id": "aGuonfLrijke"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Clustering sparse data with %s\" % km)\n",
        "t0 = time()\n",
        "km.fit(X)\n",
        "print(\"done in %0.3fs\" % (time() - t0))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ARuqSbrTilWv",
        "outputId": "4c155055-973f-4334-8b98-64a900d52824"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Clustering sparse data with MiniBatchKMeans(batch_size=1000, init_size=1000, n_clusters=4, n_init=1,\n",
            "                verbose=False)\n",
            "done in 0.253s\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Homogeneity: %0.3f\" % metrics.homogeneity_score(labels, km.labels_))\n",
        "print(\"Completeness: %0.3f\" % metrics.completeness_score(labels, km.labels_))\n",
        "print(\"V-measure: %0.3f\" % metrics.v_measure_score(labels, km.labels_))\n",
        "print(\"Adjusted Rand-Index: %.3f\" % metrics.adjusted_rand_score(labels, km.labels_))\n",
        "print(\n",
        "    \"Silhouette Coefficient: %0.3f\"\n",
        "    % metrics.silhouette_score(X, km.labels_, sample_size=1000)\n",
        ")\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7aoICccSinxp",
        "outputId": "2a72af2b-7e49-4b35-d4b0-c3f253910dc6"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Homogeneity: 0.536\n",
            "Completeness: 0.625\n",
            "V-measure: 0.577\n",
            "Adjusted Rand-Index: 0.541\n",
            "Silhouette Coefficient: 0.007\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "if not opts.use_hashing:\n",
        "    print(\"Top terms per cluster:\")\n",
        "\n",
        "    if opts.n_components:\n",
        "        original_space_centroids = svd.inverse_transform(km.cluster_centers_)\n",
        "        order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
        "    else:\n",
        "        order_centroids = km.cluster_centers_.argsort()[:, ::-1]\n",
        "\n",
        "    terms = vectorizer.get_feature_names_out()\n",
        "    for i in range(true_k):\n",
        "        print(\"Cluster %d:\" % i, end=\"\")\n",
        "        for ind in order_centroids[i, :10]:\n",
        "            print(\" %s\" % terms[ind], end=\"\")\n",
        "        print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xqzqjydZirYG",
        "outputId": "07bf0fd1-35ea-4329-f71d-ff61dcf6ab59"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Top terms per cluster:\n",
            "Cluster 0: graphics image com university thanks 3d posting file host nntp\n",
            "Cluster 1: god com people don article say keith think sgi just\n",
            "Cluster 2: sandvik kent apple newton com alink ksand cookamunga tourist private\n",
            "Cluster 3: space nasa henry access alaska toronto gov digex pat shuttle\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reference : https://scikit-learn.org/stable/auto_examples/text/plot_document_clustering.html#sphx-glr-auto-examples-text-plot-document-clustering-py"
      ],
      "metadata": {
        "id": "QvEzgX0Hob2h"
      }
    }
  ]
}